{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mltools as ml\n",
    "\n",
    "np.random.seed(0)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data\n",
    "X = np.genfromtxt('data/X_train.txt', delimiter=None)\n",
    "Y = np.genfromtxt('data/Y_train.txt', delimiter=None)\n",
    "\n",
    "# And the test features\n",
    "Xte = np.genfromtxt('data/X_test.txt', delimiter=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape, Y.shape, Xte.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All your work should be done on the training data set. To be able to make educated decisions on which classifier you're going to use, you should split it into train and validation data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr, Xva, Ytr, Yva = ml.splitData(X, Y) # Default is 80% training/20% validation\n",
    "Xtr, Ytr = ml.shuffleData(Xtr, Ytr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Xtr.shape, Xva.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking a subsample of the data so that trains faster.  You should train on whole data for the Kaggle competition.\n",
    "Xt, Yt = Xtr[:4000], Ytr[:4000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## World of Classifiers\n",
    "Time to start doing some classifications! We'll demonstrate a number of classifiers you may want to use for the final project, but may or may not have covered in class at this point. For now, we'll focus on how to use these classifiers as a black box with MLTools.\n",
    "\n",
    "**IMPORTANT NOTE:**  For the Kaggle competition, you need to submit probabilities and not just class predictions. Don't worry, you don't need to code that, just use the `predictSoft()` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "\n",
    "Decision trees will be covered in Tuesday's lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The decision tree classifier has minLeaf and maxDepth parameters. You should know what these mean by next week.\n",
    "learner = ml.dtree.treeClassify(Xt, Yt, minLeaf=30, maxDepth=20)\n",
    "\n",
    "# Prediction\n",
    "probs = learner.predictSoft(Xte)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `predictSoft` method returns an $M \\times C$ table in which for each data point you have the proability of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compute the AUC for both the training and validation data sets. For a refresher on AUC, look at the Bayes [lecture slides](https://canvas.eee.uci.edu/courses/24330/pages/lecture-calendar) or the [Wikipedia article](https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{0:>15}: {1:.4f}\".format('Train AUC', learner.auc(Xt, Yt)))\n",
    "print(\"{0:>15}: {1:.4f}\".format('Validation AUC', learner.auc(Xva, Yva)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play with different parameters to see how AUC changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing a decision tree\n",
    "The MLTools implementation of the decision tree classifier also provides a printing mechanism. However, it only works up to depth 2, so not very useful for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = ml.dtree.treeClassify()\n",
    "learner.train(Xt, Yt, maxDepth=2)\n",
    "print(learner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the data\n",
    "XtrP, params = ml.rescale(Xt)\n",
    "XteP, _ = ml.rescale(Xte, params)\n",
    "\n",
    "print(XtrP.shape, XteP.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we do **not** need to scale the data for decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "learner = ml.linearC.linearClassify()\n",
    "learner.train(XtrP, Yt, initStep=0.5, stopTol=1e-6, stopIter=100)\n",
    "\n",
    "probs = learner.predictSoft(XteP)\n",
    "print(probs[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the AUC is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{0:>15}: {1:.4f}\".format('Train AUC',learner.auc(XtrP, Yt)))\n",
    "print(\"{0:>15}: {1:.4f}\".format('Validation AUC', learner.auc(Xva, Yva)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is why we're using a validation data set. We can see already that for THIS specific configuration the decision tree is much better. It is very likely that it'll be better on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "\n",
    "Neural networks will be covered in Thursday's lecture. We'll use the MLTools implementation in our examples. Having said that, if you want to use some more fancy packages you are more than welcome to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = ml.nnet.nnetClassify()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we construct the classifier, we can define the sizes of its layers and initialize their values with `nn.init_weights`:\n",
    "\n",
    "    nn.init_weights(self, sizes, init, X, Y)\n",
    "\n",
    "From the method description (see `mltools/nnet.py`):\n",
    "- `sizes = [Ninput, N1, N2, ... , Noutput]`, where `Ninput` = # of input features, and `Nouput` = # classes\n",
    "- `init = {'zeros', 'random'}` : initialize to all zeros or small random values (breaks symmetry)\n",
    "\n",
    "Training the model using gradient descent, we can track the surrogate loss (here, MSE loss on the output vector, compared to a 1-of-K representation of the class), as well as the 0/1 classification loss (error rate):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.init_weights([14, 5, 3], 'random', Xt, Yt)\n",
    "nn.train(Xt, Yt, stopTol=1e-8, stepsize=.25, stopIter=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WHAT DID WE DO WRONG?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to specify the right number of input and output layers.\n",
    "nn.init_weights([Xt.shape[1], 5, len(np.unique(Yt))], 'random', Xt, Yt)\n",
    "nn.train(Xt, Yt, stopTol=1e-8, stepsize=.25, stopIter=50)  # Really small stopIter so it will stop fast :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{0:>15}: {1:.4f}\".format('Train AUC',nn.auc(Xt, Yt)))\n",
    "print(\"{0:>15}: {1:.4f}\".format('Validation AUC', nn.auc(Xva, Yva)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AUC results are bad because we just used a simple configuration of the NN. NN can be engineered until your last day, but some things should make sense to you.\n",
    "\n",
    "One example is the option to change the activation function. This is the function that is in the inner layers. By default, the code comes with the `tanh`, but the `logistic` (sigmoid) is also coded in and you can just specify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.setActivation('logistic')\n",
    "\n",
    "nn.train(Xt, Yt, stopTol=1e-8, stepsize=.25, stopIter=100)\n",
    "print(\"{0:>15}: {1:.4f}\".format('Train AUC',nn.auc(Xt, Yt)))\n",
    "print(\"{0:>15}: {1:.4f}\".format('Validation AUC', nn.auc(Xva, Yva)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing your own activation function\n",
    "\n",
    "Not suprisingly, you can also provide a custom activation function. Note that for the last layer you will probably always want the sigmoid function, so only change the inner layer ones.\n",
    "\n",
    "The function definition is this:\n",
    "\n",
    "    setActivation(self, method, sig=None, d_sig=None, sig_0=None, d_sig_0=None)\n",
    "    \n",
    "You can call it with `method='custom'` and then specify both `sig` and `d_sig` (the `sig_0` and `d_sig_0` ones are for the last layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's a dummy activation method (f(x) = x)\n",
    "sig = lambda z: np.atleast_2d(z)\n",
    "dsig = lambda z: np.atleast_2d(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = ml.nnet.nnetClassify()\n",
    "nn.init_weights([Xt.shape[1], 5, len(np.unique(Yt))], 'random', Xt, Yt)\n",
    "\n",
    "nn.setActivation('custom', sig, dsig)\n",
    "\n",
    "nn.train(Xt, Yt, stopTol=1e-8, stepsize=.25, stopIter=100)\n",
    "print(\"{0:>15}: {1:.4f}\".format('Train AUC',nn.auc(Xt, Yt)))\n",
    "print(\"{0:>15}: {1:.4f}\".format('Validation AUC', nn.auc(Xva, Yva)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Hyperparameters\n",
    "\n",
    "We've learned that one way of guessing how well we're doing with different model parameters is to plot the train and validation errors as a function of that parameter (e.g, $k$ in the KNN, or degree in the linear classifier and regression).\n",
    "\n",
    "Now it seems like there could be more parameters involved. One example is the `minLeaf` and the `maxDepth` values in a decision tree.\n",
    "\n",
    "When it's two parameters, you can simply use heatmaps. The $x$ and $y$ axes represent the parameters, and the \"heat\" is the validation/train error as a \"third\" dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to use a dummy function to show that. Let's assume we have two parameters `p1` and `p2`, and the prediction accuracy is `p1 + p2` (just as an example, you would use a real evaluation metric like the AUC in real applications)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = np.arange(5)\n",
    "p2 = np.arange(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = np.zeros([p1.shape[0], p2.shape[0]])\n",
    "for i in range(p1.shape[0]):\n",
    "    for j in range(p2.shape[0]):\n",
    "        acc[i][j] = p1[i] + p2[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "\n",
    "heatmap = ax.matshow(acc)\n",
    "f.colorbar(heatmap)\n",
    "\n",
    "ax.set_xticks(p1)\n",
    "ax.set_xticklabels(['a', 'b', 'c', 'd', 'e'])\n",
    "\n",
    "ax.set_yticks(p2)\n",
    "ax.set_yticklabels(['%d' % p for p in p2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting Predictions\n",
    "\n",
    "Let's assume that the last classifier we ran was the best one (after we used all that we know to verify it is the best one including that plot from the previous block). Now let's run it on the test data, and create a file that can be submitted.\n",
    "\n",
    "Each line in the file is a data point ID and the probability of $P(Y=1)$. There's also a header line. Here's how you can create it simply from the probs matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data for submission by taking the P(Y=1) column from probs and just add a running index as the first column.\n",
    "Y_sub = np.vstack([np.arange(Xte.shape[0]), probs[:, 1]]).T\n",
    "\n",
    "# We specify the header (ID, Prob1) and also specify the comments as '' so the header won't be commented out with\n",
    "# the # sign.\n",
    "np.savetxt('data/Y_sub.txt', Y_sub, '%d, %.5f', header='ID,Prob1', comments='', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to upload our predictions to the Kaggle website, and compete to top the leaderboard!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
