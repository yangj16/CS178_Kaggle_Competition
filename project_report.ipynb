{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Member Names: Jiayi Huang,  Ziru Yan,  Jing Yang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle eam Name: xswl getA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. A table listing each model, as well as your best blended/stacked model ensembles, and their performance on training and validation and leaderboard data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- K-Nearest Neighbors Classifier\n",
    "    - Training AUC: 0.8065\n",
    "    - Validation AUC: 0.6698\n",
    "    - Leaderboard AUC: 0.7364\n",
    "        \n",
    "- Random Forest Classifier\n",
    "    - Training AUC: 0.8439\n",
    "    - Validation AUC: 0.7552\n",
    "    - Leaderboard AUC: 0.75257\n",
    "\n",
    "- Logistic Regression Classifier\n",
    "    - Training AUC: 0.7839\n",
    "    - Validation AUC: 0.7229\n",
    "    - Leaderboard AUC: 0.7229\n",
    "\n",
    "- Ensemble Performance\n",
    "    - Validation AUC: 0.78247\n",
    "    - Leaderboard AUC: 0.78247"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. For each model, a paragraph or two describing:  what features you gave it (raw inputs, selected inputs, non-linear feature expansions, etc.); how was it trained (learning algorithm and software source); and key hyperparameter settings (plus your approach to choosing those settings)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- K-Nearest Neighbors Classifier  \n",
    "We created a list of kNN classifier (len=20), each one randomly choose partial raw inputs as training data and produced output prediction. The average of these 20 classifiers' outputs was the prediction of the training data, we trained it using k-nearest neighbors algorithm from sklearn.neighbors.KNeighborsClassifier. The key hyperparameter settings is n_neighbors=6, weights=\"distance\". Our approach to choose these settings is using loops to generate different combinations of the hyperparameters and choose the one with highest validation AUC.\n",
    "\n",
    "- Random Forest Classifier  \n",
    "Our random forest classifier is given sub-samples of raw inputs which have been reprocessed by using the “bootstrapdata” method in mltool. Random forest is one method to improve the accuracy of decision trees. It creates 30 sub-sample to train the model by decision tree model datas and averages 30 prediction models. For decision trees, the main idea is to increase the max depth and decrease the minParent to improve the accuracy of the prediction. The validation data MSE is almost constant after 19 and performs best at 24 and above. MinParent is chosen at 32(2**6) since it would have higher complexity but not overfitting according to the difference between training and validation MSE. The selection of  parameters is dependent on the training and validation auc difference as well. A higher validation auc with least difference is selected.\n",
    "\n",
    "- Logistic Regression Classifier  \n",
    "Our logistic Regression Classifier use 2 degree polynomial feature as the basic feature. The raw inputs are randomly shuffled data which each use cross validation techniques that split the data set into 5. The software source of the trainning is sklearn. The algorithm is linear_model.LogisticRegression(). This function would build a polynominal logistic Regression classifier. The key hyperparameter is preprocessing.PolynomialFeatures(degree=2, interaction_only=True). Our approach to this setting is using loop to trying different combination of interaction_only and degree. The test shows degree=2 is the best choice from 1 to 5.\n",
    "\n",
    "- Ensemble Classifier  \n",
    "We give different weights to different clasifier. Due to there weights, their prediction have certain probability to become the final predicition. We use logistic regression classifier weight 5, random regression classifier weight 2, and kNN classifier weight 0.5 to get our best performance classifier. This ensemble algorithm is written by ourselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. A paragraph or two describing your overall prediction ensemble:  how did you combine the individual models, and why did you pick that technique?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We combine three individual models using weighted averages. It could reduce the risk of overfitting as well as improve performance without losing any features and probabilities. Since Random Forest Classifer and Logistic Regression Classifier had higher validation AUC, they were given more weight than K-Nearest Neighbors Classifier. We pick this technique because classifier with higher validation AUC score generally had higher AUC score on the Leaderboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. A conclusion paragraph highlighting the methods/algorithms that you think worked particularly well for this data, the methods/algorithms that worked poorly, and your hypotheses as to why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the three classifiers, the best result from our prediction is the Random Forest Classifier and the worst result is kNN prediction. The result of prediction is affected by the number of features and size of the dataset. KNN is one of the simple implemented learning methods. If the dataset is small without any noise, KNN will be a good choice, since the number of K is directly related to the complexity of the model. A lower K which has more accurate fit would result in low bias but high variance, vice versa. In addition, since KNN does not have a training process, it requires a reduced or weighted number of features to compute the model. It would affect the accuracy of prediction for validation data. Therefore, KNN is not a good choice for a high dimension dataset. \n",
    "\n",
    "On the other hand, Decision Tree Classifier is able to deal with high dimension problem. Decision Tree Classifier is likely to cause overfitting. There are effective ways to avoid complexity for Decision Tree such as cross validation and ensembles. Our approach are ensemble of classifiers and using Random Forest Classifer instead of Decision Tree. Since kNN may not be a good choice, to have a better ensemble result, we add logistic regression classifier to the ensemble to get a better performance model.\n",
    "\n",
    "In conclusion, the ensemble of classifiers (weighted average) works best while single learning algorithm like kNN works poorly."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
