{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. A table listing each model, as well as your best blended/stacked model ensembles, and their performance on training and validation and leaderboard data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- K-Nearest Neighbors Classifier\n",
    "    - Training AUC: 0.8425189376095492\n",
    "    - Validation AUC: 0.6640564892048516\n",
    "    - Leaderboard AUC: \n",
    "        \n",
    "- Random Forest Classifier\n",
    "    - Training AUC: 0.8439\n",
    "    - Validation AUC: 0.7552\n",
    "    - Leaderboard AUC: 0.75257\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. For each model, a paragraph or two describing:  what features you gave it (raw inputs, selected inputs, non-linear feature expansions, etc.); how was it trained (learning algorithm and software source); and key hyperparameter settings (plus your approach to choosing those settings)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- K-Nearest Neighbors Classifier  \n",
    "We created a list of kNN classifier, each one randomly choose partial of raw inputs. we trained it using k-nearest neighbors algorithm from sklearn.neighbors.KNeighborsClassifier. The key hyperparameter settings is n_neighbors=6, weights=\"distance\". Our approach to choose these settings is listing different combinations of the hyperparameters and choose the one with highest validation AUC.\n",
    "- Random Forest Classifier  \n",
    "Our random forest classifier is given sub-samples of raw inputs which have been reprocessed by using the “bootstrapdata” method in mltool. Random forest is one method to improve the accuracy of decision trees. It creates 30 sub-sample to train the model by decision tree model datas and averages 30 prediction models. For decision trees, the main idea is to increase the max depth and decrease the minParent to improve the accuracy of the prediction. The validation data MSE is almost constant after 19 and performs best at 24 and above. MinParent is chosen at 32(2**6) since it would have higher complexity but not overfitting according to the difference between training and validation MSE. The selection of  parameters is dependent on the training and validation auc difference as well. A higher validation auc with least difference is selected.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. A paragraph or two describing your overall prediction ensemble:  how did you combine the individual models, and why did you pick that technique?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We combine the individual models by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. A conclusion paragraph highlighting the methods/algorithms that you think worked particularly well for this data, the methods/algorithms that worked poorly, and your hypotheses as to why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
